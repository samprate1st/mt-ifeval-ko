#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Langfuseì™€ LangChainì„ í™œìš©í•œ ë©€í‹°í„´ Instruction Following í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. Langfuse ì„œë²„ì—ì„œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
2. LangChainì„ ì‚¬ìš©í•˜ì—¬ ë©€í‹°í„´ ëŒ€í™” ìƒì„±
3. IFEvalì„ ì‚¬ìš©í•˜ì—¬ strict/loose í‰ê°€ ìˆ˜í–‰
4. í‰ê°€ ê²°ê³¼ë¥¼ Langfuse ì„œë²„ì— ê¸°ë¡
"""

import os
import sys
import json
import time
import uuid
import argparse
from glob import glob
from pprint import pprint
import pandas as pd
import numpy as np
import warnings
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore")

# í™˜ê²½ë³€ìˆ˜ ì„¤ì • ë° ë¡œë“œ
SCRIPT_DIR = os.path.abspath(os.path.dirname(__file__))
WORK_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, ".."))
ENV_FILE = os.path.join(WORK_DIR, ".env.params")
print(f"ENV_FILE: {ENV_FILE}")
from dotenv import load_dotenv
load_dotenv(ENV_FILE)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(WORK_DIR)

# Langfuse ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from langfuse import get_client, Langfuse
    from langfuse.langchain import CallbackHandler
except ImportError:
    print("Error: Langfuse ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”. (pip install langfuse)")
    sys.exit(1)

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

# IFEval ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    # ìƒëŒ€ ê²½ë¡œë¡œ ifeval ëª¨ë“ˆ ì„í¬íŠ¸
    sys.path.append(os.path.join(WORK_DIR, "ifeval"))
    from ifeval.evaluation_main import test_instruction_following_strict, test_instruction_following_loose
    from ifeval.evaluation_main import InputExample, OutputExample
except ImportError:
    print("Warning: ifeval ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    sys.exit(1)

# í‰ê°€ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤ë“¤
@dataclass
class EvaluationScore:
    """í‰ê°€ ì ìˆ˜ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    overall_score: float
    strict_prompt_score: float
    strict_inst_score: float
    loose_prompt_score: float
    loose_inst_score: float

@dataclass
class EvaluationDetail:
    """í‰ê°€ ì„¸ë¶€ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    follow_all_instructions: bool
    follow_instruction_list: list[bool]
    is_evaluated_list: list[bool]

@dataclass
class EvaluationResult:
    """í‰ê°€ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    item_id: str
    key: str
    turn_index: int
    input: Any
    output: str
    score: EvaluationScore
    result_strict: EvaluationDetail
    result_loose: EvaluationDetail
    trace_id: Optional[str] = None
    error: Optional[str] = None

class LangfuseEvaluator:
    """Langfuseë¥¼ ì‚¬ìš©í•œ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = None, temperature: float = 0.0, verbose: bool = False):
        """
        í‰ê°€ê¸° ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  LLM ëª¨ë¸ ì´ë¦„
            temperature: ìƒì„± ì˜¨ë„
            verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        """
        self.verbose = verbose
        
        # Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.langfuse_client = get_client()
        auth_status = self.langfuse_client.auth_check()
        print(f"Langfuse ì¸ì¦ ìƒíƒœ: {auth_status}")
        
        if not auth_status:
            print("Error: Langfuse ì¸ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            sys.exit(1)
        
        # LLM ëª¨ë¸ ì´ˆê¸°í™”
        if model_name is None:
            if os.getenv("OPENAI_BASE_URL") == "https://api.openai.com/v1":
                model_name = "gpt-4.1-mini"
            else:
                model_name = os.getenv("MODEL_NAME", "gpt-4.1-mini")
        
        self.model_name = model_name
        self.temperature = temperature
        
        print(f"LLM ëª¨ë¸: {self.model_name} (temperature={self.temperature})")
        self.llm = ChatOpenAI(model=self.model_name, temperature=self.temperature)
        
        # Langfuse ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
        self.langfuse_handler = CallbackHandler()
        
        # ì²´ì¸ ìƒì„±
        self.ifgen_chain = self._create_ifgen_chain()
    
    def _create_ifgen_chain(self):
        """IFGen ì²´ì¸ ìƒì„±"""
        template = """{prompt}"""
        prompt = ChatPromptTemplate.from_template(template)

        ifgen_chain = (
            {"prompt": RunnableLambda(self._check_prompt)} 
            | prompt
            | self.llm
            | StrOutputParser()
        )

        return ifgen_chain
    
    def _check_prompt(self, prompt):
        """í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜"""
        # ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ JSON íŒŒì‹±ì„ ì‹œë„
        if isinstance(prompt, str):
            try:
                # JSON í˜•ì‹ì¸ ê²½ìš° íŒŒì‹± ì‹œë„
                item = json.loads(prompt)
                if isinstance(item, str):
                    return item
                elif isinstance(item, dict):
                    return item.get("content")
                else:
                    raise ValueError("Invalid prompt type")
            except json.JSONDecodeError:
                # JSON í˜•ì‹ì´ ì•„ë‹Œ ì¼ë°˜ ë¬¸ìì—´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
                return prompt
        else:
            # ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš° (ì´ë¯¸ íŒŒì‹±ëœ ê°ì²´ì¸ ê²½ìš°)
            if isinstance(prompt, dict):
                return prompt.get("content", prompt)
            return prompt
    
    def invoke_with_tracing(self, prompt: str, **kwargs):
        """ì¶”ì ì´ í¬í•¨ëœ ì²´ì¸ ì‹¤í–‰"""
        return self.ifgen_chain.invoke(
            prompt, 
            config={
                "callbacks": [self.langfuse_handler],
                "metadata": {
                    "langfuse_tags": ["evaluation", "ifeval"],
                    "evaluation_run": True
                },
            },
            **kwargs
        )
    
    def gen_response_with_max_retry(self, messages: list[BaseMessage], max_retry: int = 3) -> BaseMessage:
        """ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ë¡œ ì‘ë‹µ ìƒì„±"""
        if len(messages) == 0:
            raise ValueError("messages is empty")
        
        for attempt in range(max_retry, 0, -1):
            try:
                # ë©”ì‹œì§€ ì²´ì¸ ì‹¤í–‰
                prompt = messages[-1].content if messages else ""
                ai_content = self.invoke_with_tracing(prompt)
                return AIMessage(content=ai_content)
            except Exception as e:
                print(f"   âŒ ì‹¤íŒ¨: {str(e)}")
                time.sleep(1)
        return AIMessage(content=f'[MAX_RETRY={max_retry}] Failed.')
    
    def get_dataset(self, dataset_name: str):
        """Langfuseì—ì„œ ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°"""
        try:
            print(f"ğŸ” Langfuseì—ì„œ ë°ì´í„°ì…‹ '{dataset_name}' ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
            dataset = self.langfuse_client.get_dataset(name=dataset_name)
            print(f"âœ… Langfuse ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ: {len(dataset.items)}ê°œ ì•„ì´í…œ")
            return dataset
        except Exception as e:
            print(f"âŒ Langfuse ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            sys.exit(1)
    
    def get_prompt(self, item: Any, turn_index: int) -> str:
        """ì•„ì´í…œì—ì„œ í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ"""
        try:
            prompt = item.input.get(f"turn_{turn_index}", {}).get("prompt", "")
            if prompt:
                return self._check_prompt(prompt)
            return ""
        except Exception as e:
            print(f"Warning: í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ - {e}")
            return ""
    
    def get_instruction_id_list(self, item: Any, turn_index: int) -> list[str]:
        """ì•„ì´í…œì—ì„œ instruction_id_list ì¶”ì¶œ"""
        try:
            instruction_id_list = item.input.get(f"turn_{turn_index}", {}).get("instruction_id_list", "[]")
            
            # ë¬¸ìì—´ì¸ ê²½ìš° JSONìœ¼ë¡œ íŒŒì‹±
            if isinstance(instruction_id_list, str):
                instruction_id_list = json.loads(instruction_id_list)
            
            # ì–¸ì–´ ì ‘ë‘ì‚¬ ì¶”ê°€
            prefix = "en:"
            if item.metadata.get("language") == "Korean" or str(item.metadata.get("key", "")).endswith("ko"):
                prefix = "ko:"
            
            return [prefix + str(instruction_id) for instruction_id in instruction_id_list]
        except Exception as e:
            print(f"Warning: instruction_id_list ì¶”ì¶œ ì‹¤íŒ¨ - {e}")
            return []
    
    def get_kwargs(self, item: Any, turn_index: int) -> list[dict]:
        """ì•„ì´í…œì—ì„œ kwargs ì¶”ì¶œ"""
        try:
            kwargs = item.input.get(f"turn_{turn_index}", {}).get("kwargs", "[]")
            
            # ë¬¸ìì—´ì¸ ê²½ìš° JSONìœ¼ë¡œ íŒŒì‹±
            if isinstance(kwargs, str):
                kwargs = json.loads(kwargs)
            
            # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            if not isinstance(kwargs, list):
                kwargs = [kwargs]
            
            # ê° í•­ëª© ì²˜ë¦¬
            kwargs_list = []
            for kwarg in kwargs:
                if isinstance(kwarg, dict):
                    kwargs_list.append(kwarg)
                elif isinstance(kwarg, str):
                    try:
                        parsed_kwarg = json.loads(kwarg)
                        kwargs_list.append(parsed_kwarg)
                    except json.JSONDecodeError:
                        print(f"Warning: kwargs íŒŒì‹± ì‹¤íŒ¨: {kwarg}")
                        kwargs_list.append({})
                else:
                    kwargs_list.append({})
            
            return kwargs_list
        except Exception as e:
            print(f"Warning: kwargs ì¶”ì¶œ ì‹¤íŒ¨ - {e}")
            return [{}]
    
    def create_input_example(self, item: Any, turn_index: int) -> InputExample:
        """í‰ê°€ìš© ì…ë ¥ ì˜ˆì œ ìƒì„±"""
        return InputExample(
            key=item.metadata.get("key", ""),
            prompt=self.get_prompt(item, turn_index),
            instruction_id_list=self.get_instruction_id_list(item, turn_index),
            kwargs=self.get_kwargs(item, turn_index)
        )
    
    def create_input_to_response_dict(self, item: Any, turn_index: int, response: str) -> Dict[str, str]:
        """ì…ë ¥-ì‘ë‹µ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        prompt = self.get_prompt(item, turn_index)
        return {prompt: response}
    
    def calc_score_example(self, output_strict: OutputExample, output_loose: OutputExample) -> EvaluationScore:
        """í‰ê°€ ì ìˆ˜ ê³„ì‚°"""
        strict_prompt_score = 1.0 if output_strict.follow_all_instructions else 0.0
        loose_prompt_score = 1.0 if output_loose.follow_all_instructions else 0.0
        
        # ì§€ì‹œ ëª©ë¡ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
        if len(output_strict.follow_instruction_list) > 0:
            strict_inst_score = sum(output_strict.follow_instruction_list) / len(output_strict.follow_instruction_list)
        else:
            strict_inst_score = 0.0
            
        if len(output_loose.follow_instruction_list) > 0:
            loose_inst_score = sum(output_loose.follow_instruction_list) / len(output_loose.follow_instruction_list)
        else:
            loose_inst_score = 0.0
        
        overall_score = (strict_prompt_score + loose_prompt_score + strict_inst_score + loose_inst_score) / 4
        return EvaluationScore(
            overall_score=overall_score,
            strict_prompt_score=strict_prompt_score,
            strict_inst_score=strict_inst_score,
            loose_prompt_score=loose_prompt_score,
            loose_inst_score=loose_inst_score
        )
    
    def merge_result_example(self, item, turn_index: int, output_strict: OutputExample, output_loose: OutputExample, trace_id: str) -> EvaluationResult:
        """í‰ê°€ ê²°ê³¼ ë³‘í•©"""
        input_example = self.create_input_example(item, turn_index)
        output = output_strict.response
        
        result_strict = EvaluationDetail(
            follow_all_instructions=output_strict.follow_all_instructions,
            follow_instruction_list=output_strict.follow_instruction_list,
            is_evaluated_list=output_strict.is_evaluated_list
        )
        
        result_loose = EvaluationDetail(
            follow_all_instructions=output_loose.follow_all_instructions,
            follow_instruction_list=output_loose.follow_instruction_list,
            is_evaluated_list=output_loose.is_evaluated_list
        )
        
        score = self.calc_score_example(output_strict, output_loose)
        
        return EvaluationResult(
            item_id=item.id,
            key=item.metadata.get("key", ""),
            turn_index=turn_index,
            input=input_example,
            output=output,
            trace_id=trace_id,
            score=score,
            result_strict=result_strict,
            result_loose=result_loose,
            error=None
        )
    
    def eval_process_by_turn(self, item, turn_index: int, run_name: str, messages: list[BaseMessage]) -> Tuple[EvaluationResult, list[BaseMessage]]:
        """í„´ë³„ í‰ê°€ ì²˜ë¦¬"""
        with item.run(run_name=run_name+f"_turn_{turn_index}") as root_span:
            
            # í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            prompt = self.get_prompt(item, turn_index)
            if not prompt:
                print(f"   âš ï¸ í„´ {turn_index}ì˜ í”„ë¡¬í”„íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                result = EvaluationResult(
                    item_id=item.id,
                    key=item.metadata.get("key", ""),
                    turn_index=turn_index,
                    input=None,
                    output="",
                    score=EvaluationScore(0, 0, 0, 0, 0),
                    result_strict=None,
                    result_loose=None,
                    trace_id=getattr(root_span, 'trace_id', None),
                    error=f"Empty prompt for turn {turn_index}"
                )
                return result, messages
            
            # ë©”ì‹œì§€ ì¶”ê°€ ë° ì‘ë‹µ ìƒì„±
            messages.append(HumanMessage(content=prompt))
            print(f"   ğŸ”„ í„´ {turn_index} ì²˜ë¦¬ ì¤‘...")
            
            if self.verbose:
                print("   ì…ë ¥ ë©”ì‹œì§€:")
                pprint(messages)
            
            ai_message = self.gen_response_with_max_retry(messages)
            messages.append(ai_message)
            response = ai_message.content
        
            if self.verbose:
                print("   ì¶œë ¥ ë©”ì‹œì§€:")
                pprint(messages)
                    
            if "Failed" in response:
                result = EvaluationResult(
                    item_id=item.id,
                    key=item.metadata.get("key", ""),
                    turn_index=turn_index,
                    input=self.create_input_example(item, turn_index),
                    output=response,
                    score=EvaluationScore(0, 0, 0, 0, 0),
                    result_strict=None,
                    result_loose=None,
                    trace_id=getattr(root_span, 'trace_id', None),
                    error=f"Failed to generate response: {response}"
                )
                return result, messages
            
            # í‰ê°€ ìˆ˜í–‰
            input_example = self.create_input_example(item, turn_index)
            prompt_to_response = self.create_input_to_response_dict(item, turn_index, response)
            
            # 1. strict instruction following evaluation
            output_strict = test_instruction_following_strict(input_example, prompt_to_response)
            
            # 2. loose instruction following evaluation
            output_loose = test_instruction_following_loose(input_example, prompt_to_response)
            
            # ì „ì²´ ì ìˆ˜ ê³„ì‚°
            score = self.calc_score_example(output_strict, output_loose)
            print(f"   âœ… í„´ {turn_index} í‰ê°€ ì™„ë£Œ (overall score: {score.overall_score:.2f})")
            
            # 3. í‰ê°€ ê²°ê³¼ ê¸°ë¡
            # ê° instruction ì˜ True/False ê²°ê³¼ë¥¼ commentì— ì €ì¥í•œë‹¤. ë˜í•œ, evaluated True/False ê²°ê³¼ë¥¼ ì €ì¥í•œë‹¤.
            # follow_all_instructions ëŠ” ì ˆëŒ€ë¡œ commentì— ì €ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤.
            root_span.score(name="overall", value=score.overall_score, comment=str(item.metadata.get("key", "")))
            root_span.score(name="strict_prompt_score", value=score.strict_prompt_score, comment="result: "+str(output_strict.follow_instruction_list)+", tested: "+str(output_strict.is_evaluated_list))
            root_span.score(name="loose_prompt_score", value=score.loose_prompt_score, comment="result: "+str(output_loose.follow_instruction_list)+", tested: "+str(output_loose.is_evaluated_list))
            root_span.score(name="strict_inst_score", value=score.strict_inst_score, comment="result: "+str(output_strict.follow_instruction_list)+", tested: "+str(output_strict.is_evaluated_list))
            root_span.score(name="loose_inst_score", value=score.loose_inst_score, comment="result: "+str(output_loose.follow_instruction_list)+", tested: "+str(output_loose.is_evaluated_list))

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            # Langfuse API ë³€ê²½: update_metadata -> ê°ì²´ ì—…ë°ì´íŠ¸ ì‹œ metadata í•„ë“œ ì‚¬ìš©
            root_span.update(metadata={
                "key": item.metadata.get("key", ""),
                "instruction_id_list": input_example.instruction_id_list,
                "kwargs": input_example.kwargs,
                "strict_IF_result": output_strict.follow_instruction_list,
                "strict_IF_tested": output_strict.is_evaluated_list,
                "loose_IF_result": output_loose.follow_instruction_list,
                "loose_IF_tested": output_loose.is_evaluated_list
            })
            
            # ê²°ê³¼ ì €ì¥
            result = self.merge_result_example(item, turn_index, output_strict, output_loose, getattr(root_span, 'trace_id', None))
            return result, messages
    
    def run_dataset_evaluation(self, dataset_name: str, run_name: str, limit: int = None, parallel: bool = False, max_workers: int = 4) -> List[Dict]:
        """ë°ì´í„°ì…‹ ì „ì²´ì— ëŒ€í•œ í‰ê°€ ì‹¤í–‰"""
        # ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°
        dataset = self.get_dataset(dataset_name)
        if not dataset:
            raise ValueError(f"ë°ì´í„°ì…‹ '{dataset_name}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        items = dataset.items
        if limit and limit > 0:
            items = items[:limit]
        
        print(f"ğŸ“Š multi-if í‰ê°€ ì‹œì‘: {dataset_name} ({len(items)}ê°œ í•­ëª©)")
        
        if parallel:
            print(f"ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ (ì‘ì—…ì: {max_workers}ê°œ)")
            return self._run_parallel_evaluation(items, run_name, max_workers)
        else:
            print(f"ğŸ”„ ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ")
            return self._run_sequential_evaluation(items, run_name)
    
    def _run_sequential_evaluation(self, items: List, run_name: str) -> List[Dict]:
        """ìˆœì°¨ í‰ê°€ ì‹¤í–‰"""
        results = []
        successful = 0
        failed = 0
        
        # ì•„ì´í…œ ìˆœíšŒ
        for idx, item in enumerate(items, 1):
            print(f"\nğŸ”„ ì•„ì´í…œ {idx}/{len(items)} ì²˜ë¦¬ ì¤‘...")
            
            # ë©”íƒ€ë°ì´í„° ì¶œë ¥
            print(f"   ğŸ“‹ í‚¤: {item.metadata.get('key', 'N/A')}")
            print(f"   ğŸ“‹ ì–¸ì–´: {item.metadata.get('language', 'N/A')}")
            
            try:
                item_results = self._evaluate_single_item(item, idx, run_name)
                
                # ì„±ê³µ/ì‹¤íŒ¨ í™•ì¸
                if all(result["result"].error is None for result in item_results):
                    successful += 1
                else:
                    failed += 1
                    print(f"   âŒ {item.metadata.get('key', 'N/A')} ì˜¤ë¥˜ ë°œìƒ")
                
                results.extend(item_results)
                
            except Exception as e:
                failed += 1
                print(f"   âŒ {item.metadata.get('key', 'N/A')} ì˜ˆì™¸ ë°œìƒ: {e}")
                # ì˜¤ë¥˜ í•­ëª©ë„ ê²°ê³¼ì— í¬í•¨
                results.append({
                    "item_index": idx,
                    "turn_index": 0,
                    "result": EvaluationResult(
                        item_id=item.id,
                        key=item.metadata.get("key", ""),
                        turn_index=0,
                        input=None,
                        output="",
                        score=EvaluationScore(0, 0, 0, 0, 0),
                        result_strict=None,
                        result_loose=None,
                        error=str(e)
                    )
                })

        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“‹ í‰ê°€ ì™„ë£Œ: ì„±ê³µ {successful}ê°œ / ì‹¤íŒ¨ {failed}ê°œ / ì´ {len(items)}ê°œ")
        
        return results
    
    def _run_parallel_evaluation(self, items: List, run_name: str, max_workers: int) -> List[Dict]:
        """ë³‘ë ¬ í‰ê°€ ì‹¤í–‰"""
        def evaluate_item_wrapper(item_data):
            item, idx = item_data
            try:
                return self._evaluate_single_item(item, idx, run_name)
            except Exception as e:
                print(f"   âŒ ì•„ì´í…œ {idx} ì˜ˆì™¸ ë°œìƒ: {e}")
                return [{
                    "item_index": idx,
                    "turn_index": 0,
                    "result": EvaluationResult(
                        item_id=item.id,
                        key=item.metadata.get("key", ""),
                        turn_index=0,
                        input=None,
                        output="",
                        score=EvaluationScore(0, 0, 0, 0, 0),
                        result_strict=None,
                        result_loose=None,
                        error=str(e)
                    )
                }]
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        item_data_list = [(item, idx) for idx, item in enumerate(items, 1)]
        
        # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
        results = []
        successful = 0
        failed = 0
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
            future_to_item = {executor.submit(evaluate_item_wrapper, item_data): item_data for item_data in item_data_list}
            
            for future in tqdm(future_to_item, desc="í‰ê°€ ì§„í–‰", unit="item"):
                try:
                    item_results = future.result()
                    
                    # ì„±ê³µ/ì‹¤íŒ¨ í™•ì¸
                    if all(result["result"].error is None for result in item_results):
                        successful += 1
                    else:
                        failed += 1
                    
                    results.extend(item_results)
                    
                except Exception as e:
                    failed += 1
                    item_data = future_to_item[future]
                    item, idx = item_data
                    print(f"   âŒ ì•„ì´í…œ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    
                    # ì˜¤ë¥˜ í•­ëª©ë„ ê²°ê³¼ì— í¬í•¨
                    results.append({
                        "item_index": idx,
                        "turn_index": 0,
                        "result": EvaluationResult(
                            item_id=item.id,
                            key=item.metadata.get("key", ""),
                            turn_index=0,
                            input=None,
                            output="",
                            score=EvaluationScore(0, 0, 0, 0, 0),
                            result_strict=None,
                            result_loose=None,
                            error=str(e)
                        )
                    })

        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“‹ ë³‘ë ¬ í‰ê°€ ì™„ë£Œ: ì„±ê³µ {successful}ê°œ / ì‹¤íŒ¨ {failed}ê°œ / ì´ {len(items)}ê°œ")
        
        return results
    
    def _evaluate_single_item(self, item, idx: int, run_name: str) -> List[Dict]:
        """ë‹¨ì¼ ì•„ì´í…œ í‰ê°€"""
        if not self.verbose:
            print(f"ğŸ”„ ì•„ì´í…œ {idx} ì²˜ë¦¬ ì¤‘... (í‚¤: {item.metadata.get('key', 'N/A')})")
        else:
            print(f"\nğŸ”„ ì•„ì´í…œ {idx} ì²˜ë¦¬ ì¤‘...")
            print(f"   ğŸ“‹ í‚¤: {item.metadata.get('key', 'N/A')}")
            print(f"   ğŸ“‹ ì–¸ì–´: {item.metadata.get('language', 'N/A')}")
        
        messages = []
        item_results = []
        
        # í„´ ìˆ˜ í™•ì¸
        max_turn = 0
        for key in item.input:
            if key.startswith("turn_") and key[5:].isdigit():
                turn_num = int(key[5:])
                max_turn = max(max_turn, turn_num)
        
        if max_turn == 0:
            if self.verbose:
                print(f"   âš ï¸ ì•„ì´í…œì— í„´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ê° í„´ë³„ë¡œ í‰ê°€ ìˆ˜í–‰
        with item.run(run_name=run_name) as root_span_all:

            success_turns = []
            for turn_index in range(1, max_turn + 1):
                result, messages = self.eval_process_by_turn(item, turn_index, run_name, messages)

                if result.error is None:
                    success_turns.append(True)
                else:
                    success_turns.append(False)
                    if self.verbose:
                        print(f"   âŒ {item.metadata.get('key', 'N/A')} ì˜¤ë¥˜: {result.error}")

                item_results.append({
                    "item_index": idx,
                    "turn_index": turn_index,
                    "result": result
                })

            # í‰ê°€ ê²°ê³¼ ê¸°ë¡
            score = EvaluationScore(0, 0, 0, 0, 0)
            for result in item_results:
                score.overall_score += result["result"].score.overall_score
                score.strict_prompt_score += result["result"].score.strict_prompt_score
                score.loose_prompt_score += result["result"].score.loose_prompt_score
                score.strict_inst_score += result["result"].score.strict_inst_score
                score.loose_inst_score += result["result"].score.loose_inst_score

            if item_results:
                score.overall_score /= len(item_results)
                score.strict_prompt_score /= len(item_results)
                score.loose_prompt_score /= len(item_results)
                score.strict_inst_score /= len(item_results)
                score.loose_inst_score /= len(item_results)

            root_span_all.score(name="overall", value=score.overall_score, comment=f"í‰ê·  ì ìˆ˜: {len(item_results)}ê°œ í„´")
            root_span_all.score(name="strict_prompt_score", value=score.strict_prompt_score, comment=f"í‰ê·  ì ìˆ˜: {len(item_results)}ê°œ í„´")
            root_span_all.score(name="loose_prompt_score", value=score.loose_prompt_score, comment=f"í‰ê·  ì ìˆ˜: {len(item_results)}ê°œ í„´")
            root_span_all.score(name="strict_inst_score", value=score.strict_inst_score, comment=f"í‰ê·  ì ìˆ˜: {len(item_results)}ê°œ í„´")
            root_span_all.score(name="loose_inst_score", value=score.loose_inst_score, comment=f"í‰ê·  ì ìˆ˜: {len(item_results)}ê°œ í„´")

            # ì „ì²´ ì•„ì´í…œì— ëŒ€í•œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            all_metadata = {
                "key": item.metadata.get("key", ""),
                "language": item.metadata.get("language", ""),
                "total_turns": len(item_results),
                "turn_results": [
                    {
                        "turn_index": result["turn_index"],
                        "overall_score": result["result"].score.overall_score,
                        "strict_prompt_score": result["result"].score.strict_prompt_score,
                        "loose_prompt_score": result["result"].score.loose_prompt_score,
                        "strict_inst_score": result["result"].score.strict_inst_score,
                        "loose_inst_score": result["result"].score.loose_inst_score,
                        
                        "strict_IF_result": result["result"].result_strict.follow_instruction_list,
                        "strict_IF_tested": result["result"].result_strict.is_evaluated_list,
                        "loose_IF_result": result["result"].result_loose.follow_instruction_list,
                        "loose_IF_tested": result["result"].result_loose.is_evaluated_list
                    } for result in item_results
                ]
            }
            root_span_all.update(metadata=all_metadata)
            
            return item_results
    
    def save_results_to_file(self, results: List[Dict], output_path: str):
        """í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # ê²°ê³¼ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            serializable_results = []
            for result_item in results:
                item_dict = {
                    "item_index": result_item["item_index"],
                    "turn_index": result_item["turn_index"],
                }
                
                result = result_item["result"]
                result_dict = {
                    "item_id": result.item_id,
                    "key": result.key,
                    "turn_index": result.turn_index,
                    "input": result.input.prompt,
                    "output": result.output,
                    "trace_id": result.trace_id,
                    "error": result.error
                }
                
                # ì ìˆ˜ ì •ë³´
                if result.score:
                    result_dict["score"] = {
                        "overall_score": result.score.overall_score,
                        "strict_prompt_score": result.score.strict_prompt_score,
                        "strict_inst_score": result.score.strict_inst_score,
                        "loose_prompt_score": result.score.loose_prompt_score,
                        "loose_inst_score": result.score.loose_inst_score
                    }
                
                # strict í‰ê°€ ê²°ê³¼
                if result.result_strict:
                    result_dict["result_strict"] = {
                        "follow_all_instructions": result.result_strict.follow_all_instructions,
                        "follow_instruction_list": result.result_strict.follow_instruction_list,
                        "is_evaluated_list": result.result_strict.is_evaluated_list
                    }
                
                # loose í‰ê°€ ê²°ê³¼
                if result.result_loose:
                    result_dict["result_loose"] = {
                        "follow_all_instructions": result.result_loose.follow_all_instructions,
                        "follow_instruction_list": result.result_loose.follow_instruction_list,
                        "is_evaluated_list": result.result_loose.is_evaluated_list
                    }
                
                item_dict["result"] = result_dict
                serializable_results.append(item_dict)
            
            # ê²°ê³¼ ì €ì¥
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
            
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Langfuseì™€ LangChainì„ í™œìš©í•œ ë©€í‹°í„´ Instruction Following í‰ê°€")
    parser.add_argument("--dataset", "-d", type=str, required=True, help="í‰ê°€í•  Langfuse ë°ì´í„°ì…‹ ì´ë¦„")
    parser.add_argument("--run-name", "-r", type=str, default=f"ifeval_{int(time.time())}", help="í‰ê°€ ì‹¤í–‰ ì´ë¦„")
    parser.add_argument("--model", "-m", type=str, help="ì‚¬ìš©í•  LLM ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--temperature", "-t", type=float, default=0.6, help="ìƒì„± ì˜¨ë„")
    parser.add_argument("--limit", "-l", type=int, help="í‰ê°€í•  ìµœëŒ€ ì•„ì´í…œ ìˆ˜")
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
    parser.add_argument("--output", "-o", type=str, help="ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--parallel", "-p", action="store_true", help="ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©")
    parser.add_argument("--workers", "-w", type=int, default=4, help="ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—…ì ìˆ˜")
    
    args = parser.parse_args()
    
    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = LangfuseEvaluator(
        model_name=args.model,
        temperature=args.temperature,
        verbose=args.verbose
    )
    
    # í‰ê°€ ì‹¤í–‰
    results = evaluator.run_dataset_evaluation(
        dataset_name=args.dataset,
        run_name=args.run_name,
        limit=args.limit,
        parallel=args.parallel,
        max_workers=args.workers
    )
    
    # ê²°ê³¼ ì €ì¥
    if args.output:
        evaluator.save_results_to_file(results, args.output)
    
    print("âœ… í‰ê°€ ì™„ë£Œ")

if __name__ == "__main__":
    main() 