#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Langfuseì™€ LangGraphë¥¼ í™œìš©í•œ ë©€í‹°í„´ Instruction Following í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. Langfuse ì„œë²„ì—ì„œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
2. LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ë©€í‹°í„´ ëŒ€í™” ìƒì„± ë° í‰ê°€ ì›Œí¬í”Œë¡œìš° êµ¬ì¶•
3. IFEvalì„ ì‚¬ìš©í•˜ì—¬ strict/loose í‰ê°€ ìˆ˜í–‰
4. í‰ê°€ ê²°ê³¼ë¥¼ Langfuse ì„œë²„ì— ê¸°ë¡

LangGraph ìµœì í™” íŠ¹ì§•:
- ìƒíƒœ ê´€ë¦¬ë¥¼ í†µí•œ ë©€í‹°í„´ ëŒ€í™” ì²˜ë¦¬
- ì¡°ê±´ë¶€ ë…¸ë“œë¥¼ í†µí•œ íš¨ìœ¨ì ì¸ ì›Œí¬í”Œë¡œìš° ì œì–´
- ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›
- ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
"""

import os
import sys
import json
import time
import uuid
import argparse
from glob import glob
from pprint import pprint
import pandas as pd
import numpy as np
import warnings
from dataclasses import dataclass, asdict
from typing import Any, Dict, List, Tuple, Optional, TypedDict, Annotated, Literal
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
import asyncio
from functools import partial

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore")

# í™˜ê²½ë³€ìˆ˜ ì„¤ì • ë° ë¡œë“œ
SCRIPT_DIR = os.path.abspath(os.path.dirname(__file__))
WORK_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, ".."))
ENV_FILE = os.path.join(WORK_DIR, ".env.params")
print(f"ENV_FILE: {ENV_FILE}")
from dotenv import load_dotenv
load_dotenv(ENV_FILE)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(WORK_DIR)

# Langfuse ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from langfuse import get_client, Langfuse
    from langfuse.langchain import CallbackHandler
except ImportError:
    print("Error: Langfuse ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”. (pip install langfuse)")
    sys.exit(1)

# LangGraph ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from langgraph.graph import StateGraph, END, START
    from langgraph.prebuilt import ToolNode
    from langgraph.checkpoint.memory import MemorySaver
    from langgraph.graph.message import add_messages
except ImportError:
    print("Error: LangGraph ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”. (pip install langgraph)")
    sys.exit(1)

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

# IFEval ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from ifeval.evaluation_main import test_instruction_following_strict, test_instruction_following_loose
    from ifeval.evaluation_main import InputExample, OutputExample
except ImportError:
    print("Warning: ifeval ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    sys.exit(1)

# í‰ê°€ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤ë“¤
@dataclass
class EvaluationScore:
    """í‰ê°€ ì ìˆ˜ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    overall_score: float
    strict_prompt_score: float
    strict_inst_score: float
    loose_prompt_score: float
    loose_inst_score: float

@dataclass
class EvaluationDetail:
    """í‰ê°€ ì„¸ë¶€ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    follow_all_instructions: bool
    follow_instruction_list: list[bool]
    is_evaluated_list: list[bool]

@dataclass
class EvaluationResult:
    """í‰ê°€ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    item_id: str
    key: str
    turn_index: int
    input: Any
    output: str
    score: EvaluationScore
    result_strict: EvaluationDetail
    result_loose: EvaluationDetail
    trace_id: Optional[str] = None
    error: Optional[str] = None

# LangGraph ìƒíƒœ ì •ì˜
class GraphState(TypedDict):
    """LangGraph ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤"""
    # ë°ì´í„°ì…‹ ë° ì•„ì´í…œ ì •ë³´
    dataset_item: Any
    item_index: int
    
    # ëŒ€í™” ìƒíƒœ
    messages: Annotated[List[BaseMessage], add_messages]
    current_turn: int
    max_turns: int
    
    # í‰ê°€ ê´€ë ¨
    evaluation_results: List[EvaluationResult]
    current_prompt: str
    current_response: str
    
    # ë©”íƒ€ë°ì´í„°
    run_name: str
    trace_id: Optional[str]
    
    # ì—ëŸ¬ ì²˜ë¦¬
    error: Optional[str]
    retry_count: int
    max_retries: int

class LangGraphEvaluator:
    """LangGraphë¥¼ ì‚¬ìš©í•œ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = None, temperature: float = 0.0, verbose: bool = False):
        """
        í‰ê°€ê¸° ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  LLM ëª¨ë¸ ì´ë¦„
            temperature: ìƒì„± ì˜¨ë„
            verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        """
        self.verbose = verbose
        
        # Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.langfuse_client = get_client()
        auth_status = self.langfuse_client.auth_check()
        print(f"Langfuse ì¸ì¦ ìƒíƒœ: {auth_status}")
        
        if not auth_status:
            print("Error: Langfuse ì¸ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            sys.exit(1)
        
        # LLM ëª¨ë¸ ì´ˆê¸°í™”
        if model_name is None:
            if os.getenv("OPENAI_BASE_URL") == "https://api.openai.com/v1":
                model_name = "gpt-4o-mini"
            else:
                model_name = os.getenv("MODEL_NAME", "gpt-4o-mini")
        
        self.model_name = model_name
        self.temperature = temperature
        
        print(f"LLM ëª¨ë¸: {self.model_name} (temperature={self.temperature})")
        self.llm = ChatOpenAI(model=self.model_name, temperature=self.temperature)
        
        # Langfuse ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
        self.langfuse_handler = CallbackHandler()
        
        # LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±
        self.workflow = self._create_workflow()
    
    def _create_workflow(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±"""
        workflow = StateGraph(GraphState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("initialize_evaluation", self._initialize_evaluation)
        workflow.add_node("prepare_turn", self._prepare_turn)
        workflow.add_node("generate_response", self._generate_response)
        workflow.add_node("evaluate_response", self._evaluate_response)
        workflow.add_node("record_results", self._record_results)
        workflow.add_node("check_completion", self._check_completion)
        workflow.add_node("finalize_evaluation", self._finalize_evaluation)
        
        # ì—£ì§€ ì¶”ê°€
        workflow.add_edge(START, "initialize_evaluation")
        workflow.add_edge("initialize_evaluation", "prepare_turn")
        workflow.add_edge("prepare_turn", "generate_response")
        workflow.add_edge("generate_response", "evaluate_response")
        workflow.add_edge("evaluate_response", "record_results")
        workflow.add_edge("record_results", "check_completion")
        
        # ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
        workflow.add_conditional_edges(
            "check_completion",
            self._should_continue,
            {
                "continue": "prepare_turn",
                "finalize": "finalize_evaluation"
            }
        )
        
        workflow.add_edge("finalize_evaluation", END)
        
        # ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
        memory = MemorySaver()
        
        return workflow.compile(checkpointer=memory)
    
    def _initialize_evaluation(self, state: GraphState) -> GraphState:
        """í‰ê°€ ì´ˆê¸°í™”"""
        item = state["dataset_item"]
        
        # í„´ ìˆ˜ í™•ì¸
        max_turns = 0
        for key in item.input:
            if key.startswith("turn_") and key[5:].isdigit():
                turn_num = int(key[5:])
                max_turns = max(max_turns, turn_num)
        
        state.update({
            "current_turn": 1,
            "max_turns": max_turns,
            "messages": [],
            "evaluation_results": [],
            "retry_count": 0,
            "max_retries": 3,
            "error": None
        })
        
        if self.verbose:
            print(f"   ğŸ”„ í‰ê°€ ì´ˆê¸°í™”: ìµœëŒ€ {max_turns}í„´")
        
        return state
    
    def _prepare_turn(self, state: GraphState) -> GraphState:
        """í„´ ì¤€ë¹„"""
        item = state["dataset_item"]
        turn_index = state["current_turn"]
        
        # í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
        prompt = self.get_prompt(item, turn_index)
        
        if not prompt:
            state["error"] = f"Empty prompt for turn {turn_index}"
            return state
        
        state["current_prompt"] = prompt
        
        if self.verbose:
            print(f"   ğŸ”„ í„´ {turn_index} ì¤€ë¹„ ì™„ë£Œ")
        
        return state
    
    def _generate_response(self, state: GraphState) -> GraphState:
        """ì‘ë‹µ ìƒì„±"""
        try:
            prompt = state["current_prompt"]
            messages = state["messages"].copy()
            
            # ìƒˆ ë©”ì‹œì§€ ì¶”ê°€
            messages.append(HumanMessage(content=prompt))
            
            # LLM í˜¸ì¶œ
            response = self.llm.invoke(
                messages,
                config={
                    "callbacks": [self.langfuse_handler],
                    "metadata": {
                        "langfuse_tags": ["evaluation", "ifeval", "langgraph"],
                        "evaluation_run": True,
                        "turn_index": state["current_turn"]
                    }
                }
            )
            
            ai_message = AIMessage(content=response.content)
            messages.append(ai_message)
            
            state.update({
                "messages": messages,
                "current_response": response.content,
                "retry_count": 0
            })
            
            if self.verbose:
                print(f"   âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ (í„´ {state['current_turn']})")
            
        except Exception as e:
            state["retry_count"] += 1
            if state["retry_count"] < state["max_retries"]:
                print(f"   âš ï¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨, ì¬ì‹œë„ ({state['retry_count']}/{state['max_retries']}): {e}")
                time.sleep(1)
            else:
                state["error"] = f"Failed to generate response after {state['max_retries']} attempts: {e}"
                state["current_response"] = f"[MAX_RETRY={state['max_retries']}] Failed."
        
        return state
    
    def _evaluate_response(self, state: GraphState) -> GraphState:
        """ì‘ë‹µ í‰ê°€"""
        if state.get("error"):
            return state
        
        item = state["dataset_item"]
        turn_index = state["current_turn"]
        response = state["current_response"]
        
        try:
            # ì…ë ¥ ì˜ˆì œ ìƒì„±
            input_example = self.create_input_example(item, turn_index)
            prompt_to_response = self.create_input_to_response_dict(item, turn_index, response)
            
            # í‰ê°€ ìˆ˜í–‰
            output_strict = test_instruction_following_strict(input_example, prompt_to_response)
            output_loose = test_instruction_following_loose(input_example, prompt_to_response)
            
            # ì ìˆ˜ ê³„ì‚°
            score = self.calc_score_example(output_strict, output_loose)
            
            # ê²°ê³¼ ìƒì„±
            result = self.merge_result_example(item, turn_index, output_strict, output_loose, state.get("trace_id"))
            
            # ê²°ê³¼ ì¶”ê°€
            state["evaluation_results"].append(result)
            
            if self.verbose:
                print(f"   âœ… í‰ê°€ ì™„ë£Œ (í„´ {turn_index}, ì ìˆ˜: {score.overall_score:.2f})")
            
        except Exception as e:
            state["error"] = f"Evaluation failed for turn {turn_index}: {e}"
            print(f"   âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
        
        return state
    
    def _record_results(self, state: GraphState) -> GraphState:
        """ê²°ê³¼ ê¸°ë¡"""
        if state.get("error") or not state["evaluation_results"]:
            return state
        
        item = state["dataset_item"]
        turn_index = state["current_turn"]
        
        try:
            # í˜„ì¬ í„´ì˜ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            current_result = state["evaluation_results"][-1]
            
            # Langfuseì— ê²°ê³¼ ê¸°ë¡ - í„´ë³„ íŠ¸ë ˆì´ìŠ¤ ìƒì„±
            with item.run(run_name=f"{state['run_name']}_turn_{turn_index}") as span:
                score = current_result.score
                
                # ì ìˆ˜ ê¸°ë¡ - LangChain ë²„ì „ê³¼ ë™ì¼í•œ í˜•ì‹
                span.score(name="overall", value=score.overall_score, comment=str(item.metadata.get("key", "")))
                span.score(name="strict_prompt_score", value=score.strict_prompt_score, 
                          comment="result: "+str(current_result.result_strict.follow_instruction_list)+", tested: "+str(current_result.result_strict.is_evaluated_list))
                span.score(name="loose_prompt_score", value=score.loose_prompt_score,
                          comment="result: "+str(current_result.result_loose.follow_instruction_list)+", tested: "+str(current_result.result_loose.is_evaluated_list))
                span.score(name="strict_inst_score", value=score.strict_inst_score,
                          comment="result: "+str(current_result.result_strict.follow_instruction_list)+", tested: "+str(current_result.result_strict.is_evaluated_list))
                span.score(name="loose_inst_score", value=score.loose_inst_score,
                          comment="result: "+str(current_result.result_loose.follow_instruction_list)+", tested: "+str(current_result.result_loose.is_evaluated_list))
                
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ - LangChain ë²„ì „ê³¼ ë™ì¼í•œ í˜•ì‹
                span.update(metadata={
                    "key": item.metadata.get("key", ""),
                    "instruction_id_list": current_result.input.instruction_id_list,
                    "kwargs": current_result.input.kwargs,
                    "strict_IF_result": current_result.result_strict.follow_instruction_list,
                    "strict_IF_tested": current_result.result_strict.is_evaluated_list,
                    "loose_IF_result": current_result.result_loose.follow_instruction_list,
                    "loose_IF_tested": current_result.result_loose.is_evaluated_list
                })
                
                # trace_id ì—…ë°ì´íŠ¸
                if hasattr(span, 'trace_id'):
                    current_result.trace_id = span.trace_id
                    state["trace_id"] = span.trace_id
            
            if self.verbose:
                print(f"   ğŸ“Š ê²°ê³¼ ê¸°ë¡ ì™„ë£Œ (í„´ {turn_index})")
                
        except Exception as e:
            print(f"   âš ï¸ ê²°ê³¼ ê¸°ë¡ ì‹¤íŒ¨: {e}")
        
        return state
    
    def _check_completion(self, state: GraphState) -> GraphState:
        """ì™„ë£Œ í™•ì¸"""
        state["current_turn"] += 1
        return state
    
    def _should_continue(self, state: GraphState) -> Literal["continue", "finalize"]:
        """ê³„ì† ì§„í–‰í• ì§€ ê²°ì •"""
        if state.get("error"):
            return "finalize"
        
        if state["current_turn"] <= state["max_turns"]:
            return "continue"
        else:
            return "finalize"
    
    def _finalize_evaluation(self, state: GraphState) -> GraphState:
        """í‰ê°€ ì™„ë£Œ"""
        item = state["dataset_item"]
        results = state["evaluation_results"]
        
        try:
            # ì „ì²´ ê²°ê³¼ ìš”ì•½ - ì „ì²´ ì•„ì´í…œì— ëŒ€í•œ ë©”ì¸ íŠ¸ë ˆì´ìŠ¤ ìƒì„±
            with item.run(run_name=state["run_name"]) as root_span:
                if results:
                    # í‰ê·  ì ìˆ˜ ê³„ì‚°
                    avg_score = EvaluationScore(0, 0, 0, 0, 0)
                    for result in results:
                        avg_score.overall_score += result.score.overall_score
                        avg_score.strict_prompt_score += result.score.strict_prompt_score
                        avg_score.loose_prompt_score += result.score.loose_prompt_score
                        avg_score.strict_inst_score += result.score.strict_inst_score
                        avg_score.loose_inst_score += result.score.loose_inst_score
                    
                    count = len(results)
                    avg_score.overall_score /= count
                    avg_score.strict_prompt_score /= count
                    avg_score.loose_prompt_score /= count
                    avg_score.strict_inst_score /= count
                    avg_score.loose_inst_score /= count
                    
                    # ì „ì²´ ì ìˆ˜ ê¸°ë¡ - LangChain ë²„ì „ê³¼ ë™ì¼í•œ í˜•ì‹
                    root_span.score(name="overall", value=avg_score.overall_score, comment=f"í‰ê·  ì ìˆ˜: {count}ê°œ í„´")
                    root_span.score(name="strict_prompt_score", value=avg_score.strict_prompt_score, comment=f"í‰ê·  ì ìˆ˜: {count}ê°œ í„´")
                    root_span.score(name="loose_prompt_score", value=avg_score.loose_prompt_score, comment=f"í‰ê·  ì ìˆ˜: {count}ê°œ í„´")
                    root_span.score(name="strict_inst_score", value=avg_score.strict_inst_score, comment=f"í‰ê·  ì ìˆ˜: {count}ê°œ í„´")
                    root_span.score(name="loose_inst_score", value=avg_score.loose_inst_score, comment=f"í‰ê·  ì ìˆ˜: {count}ê°œ í„´")
                    
                    # ì „ì²´ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ - LangChain ë²„ì „ê³¼ ë™ì¼í•œ í˜•ì‹
                    all_metadata = {
                        "key": item.metadata.get("key", ""),
                        "language": item.metadata.get("language", ""),
                        "total_turns": count,
                        "turn_results": [
                            {
                                "turn_index": result.turn_index,
                                "overall_score": result.score.overall_score,
                                "strict_prompt_score": result.score.strict_prompt_score,
                                "loose_prompt_score": result.score.loose_prompt_score,
                                "strict_inst_score": result.score.strict_inst_score,
                                "loose_inst_score": result.score.loose_inst_score,
                                "strict_IF_result": result.result_strict.follow_instruction_list,
                                "strict_IF_tested": result.result_strict.is_evaluated_list,
                                "loose_IF_result": result.result_loose.follow_instruction_list,
                                "loose_IF_tested": result.result_loose.is_evaluated_list
                            } for result in results
                        ]
                    }
                    root_span.update(metadata=all_metadata)
                    
                    if self.verbose:
                        print(f"   ğŸ“Š ì „ì²´ í‰ê°€ ì™„ë£Œ (í‰ê·  ì ìˆ˜: {avg_score.overall_score:.2f})")
                
        except Exception as e:
            print(f"   âš ï¸ ìµœì¢… ê²°ê³¼ ê¸°ë¡ ì‹¤íŒ¨: {e}")
        
        return state
    
    def get_dataset(self, dataset_name: str):
        """Langfuseì—ì„œ ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°"""
        try:
            print(f"ğŸ” Langfuseì—ì„œ ë°ì´í„°ì…‹ '{dataset_name}' ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
            dataset = self.langfuse_client.get_dataset(name=dataset_name)
            print(f"âœ… Langfuse ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ: {len(dataset.items)}ê°œ ì•„ì´í…œ")
            return dataset
        except Exception as e:
            print(f"âŒ Langfuse ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            sys.exit(1)
    
    def get_prompt(self, item: Any, turn_index: int) -> str:
        """ì•„ì´í…œì—ì„œ í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ"""
        try:
            prompt = item.input.get(f"turn_{turn_index}", {}).get("prompt", "")
            if prompt:
                return self._check_prompt(prompt)
            return ""
        except Exception as e:
            print(f"Warning: í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ - {e}")
            return ""
    
    def _check_prompt(self, prompt):
        """í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜"""
        if isinstance(prompt, str):
            try:
                item = json.loads(prompt)
                if isinstance(item, str):
                    return item
                elif isinstance(item, dict):
                    return item.get("content")
                else:
                    raise ValueError("Invalid prompt type")
            except json.JSONDecodeError:
                return prompt
        else:
            if isinstance(prompt, dict):
                return prompt.get("content", prompt)
            return prompt
    
    def get_instruction_id_list(self, item: Any, turn_index: int) -> list[str]:
        """ì•„ì´í…œì—ì„œ instruction_id_list ì¶”ì¶œ"""
        try:
            instruction_id_list = item.input.get(f"turn_{turn_index}", {}).get("instruction_id_list", "[]")
            
            if isinstance(instruction_id_list, str):
                instruction_id_list = json.loads(instruction_id_list)
            
            prefix = "en:"
            if item.metadata.get("language") == "Korean" or str(item.metadata.get("key", "")).endswith("ko"):
                prefix = "ko:"
            
            return [prefix + str(instruction_id) for instruction_id in instruction_id_list]
        except Exception as e:
            print(f"Warning: instruction_id_list ì¶”ì¶œ ì‹¤íŒ¨ - {e}")
            return []
    
    def get_kwargs(self, item: Any, turn_index: int) -> list[dict]:
        """ì•„ì´í…œì—ì„œ kwargs ì¶”ì¶œ"""
        try:
            kwargs = item.input.get(f"turn_{turn_index}", {}).get("kwargs", "[]")
            
            if isinstance(kwargs, str):
                kwargs = json.loads(kwargs)
            
            if not isinstance(kwargs, list):
                kwargs = [kwargs]
            
            kwargs_list = []
            for kwarg in kwargs:
                if isinstance(kwarg, dict):
                    kwargs_list.append(kwarg)
                elif isinstance(kwarg, str):
                    try:
                        parsed_kwarg = json.loads(kwarg)
                        kwargs_list.append(parsed_kwarg)
                    except json.JSONDecodeError:
                        print(f"Warning: kwargs íŒŒì‹± ì‹¤íŒ¨: {kwarg}")
                        kwargs_list.append({})
                else:
                    kwargs_list.append({})
            
            return kwargs_list
        except Exception as e:
            print(f"Warning: kwargs ì¶”ì¶œ ì‹¤íŒ¨ - {e}")
            return [{}]
    
    def create_input_example(self, item: Any, turn_index: int) -> InputExample:
        """í‰ê°€ìš© ì…ë ¥ ì˜ˆì œ ìƒì„±"""
        return InputExample(
            key=item.metadata.get("key", ""),
            prompt=self.get_prompt(item, turn_index),
            instruction_id_list=self.get_instruction_id_list(item, turn_index),
            kwargs=self.get_kwargs(item, turn_index)
        )
    
    def create_input_to_response_dict(self, item: Any, turn_index: int, response: str) -> Dict[str, str]:
        """ì…ë ¥-ì‘ë‹µ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        prompt = self.get_prompt(item, turn_index)
        return {prompt: response}
    
    def calc_score_example(self, output_strict: OutputExample, output_loose: OutputExample) -> EvaluationScore:
        """í‰ê°€ ì ìˆ˜ ê³„ì‚°"""
        strict_prompt_score = 1.0 if output_strict.follow_all_instructions else 0.0
        loose_prompt_score = 1.0 if output_loose.follow_all_instructions else 0.0
        
        if len(output_strict.follow_instruction_list) > 0:
            strict_inst_score = sum(output_strict.follow_instruction_list) / len(output_strict.follow_instruction_list)
        else:
            strict_inst_score = 0.0
            
        if len(output_loose.follow_instruction_list) > 0:
            loose_inst_score = sum(output_loose.follow_instruction_list) / len(output_loose.follow_instruction_list)
        else:
            loose_inst_score = 0.0
        
        overall_score = (strict_prompt_score + loose_prompt_score + strict_inst_score + loose_inst_score) / 4
        return EvaluationScore(
            overall_score=overall_score,
            strict_prompt_score=strict_prompt_score,
            strict_inst_score=strict_inst_score,
            loose_prompt_score=loose_prompt_score,
            loose_inst_score=loose_inst_score
        )
    
    def merge_result_example(self, item, turn_index: int, output_strict: OutputExample, output_loose: OutputExample, trace_id: str) -> EvaluationResult:
        """í‰ê°€ ê²°ê³¼ ë³‘í•©"""
        input_example = self.create_input_example(item, turn_index)
        output = output_strict.response
        
        result_strict = EvaluationDetail(
            follow_all_instructions=output_strict.follow_all_instructions,
            follow_instruction_list=output_strict.follow_instruction_list,
            is_evaluated_list=output_strict.is_evaluated_list
        )
        
        result_loose = EvaluationDetail(
            follow_all_instructions=output_loose.follow_all_instructions,
            follow_instruction_list=output_loose.follow_instruction_list,
            is_evaluated_list=output_loose.is_evaluated_list
        )
        
        score = self.calc_score_example(output_strict, output_loose)
        
        return EvaluationResult(
            item_id=item.id,
            key=item.metadata.get("key", ""),
            turn_index=turn_index,
            input=input_example,
            output=output,
            trace_id=trace_id,
            score=score,
            result_strict=result_strict,
            result_loose=result_loose,
            error=None
        )
    
    def run_dataset_evaluation(self, dataset_name: str, run_name: str, limit: int = None, parallel: bool = False, max_workers: int = 4) -> List[Dict]:
        """ë°ì´í„°ì…‹ ì „ì²´ì— ëŒ€í•œ í‰ê°€ ì‹¤í–‰"""
        # ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°
        dataset = self.get_dataset(dataset_name)
        if not dataset:
            raise ValueError(f"ë°ì´í„°ì…‹ '{dataset_name}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        items = dataset.items
        if limit and limit > 0:
            items = items[:limit]
        
        print(f"ğŸ“Š LangGraph multi-if í‰ê°€ ì‹œì‘: {dataset_name} ({len(items)}ê°œ í•­ëª©)")
        
        results = []
        successful = 0
        failed = 0
        
        # ë³‘ë ¬ ì²˜ë¦¬ ë˜ëŠ” ìˆœì°¨ ì²˜ë¦¬
        if parallel:
            results = self._run_parallel_evaluation(items, run_name, max_workers)
        else:
            results = self._run_sequential_evaluation(items, run_name)
        
        # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„ ê³„ì‚°
        for result in results:
            if result.get("error"):
                failed += 1
            else:
                successful += 1
        
        print(f"\nğŸ“‹ í‰ê°€ ì™„ë£Œ: ì„±ê³µ {successful}ê°œ / ì‹¤íŒ¨ {failed}ê°œ / ì´ {len(items)}ê°œ")
        
        return results
    
    def _run_sequential_evaluation(self, items: List, run_name: str) -> List[Dict]:
        """ìˆœì°¨ í‰ê°€ ì‹¤í–‰"""
        results = []
        
        for idx, item in enumerate(items, 1):
            print(f"\nğŸ”„ ì•„ì´í…œ {idx}/{len(items)} ì²˜ë¦¬ ì¤‘...")
            print(f"   ğŸ“‹ í‚¤: {item.metadata.get('key', 'N/A')}")
            print(f"   ğŸ“‹ ì–¸ì–´: {item.metadata.get('language', 'N/A')}")
            
            try:
                # ì´ˆê¸° ìƒíƒœ ì„¤ì •
                initial_state = {
                    "dataset_item": item,
                    "item_index": idx,
                    "run_name": run_name,
                    "messages": [],
                    "evaluation_results": [],
                    "current_turn": 1,
                    "max_turns": 0,
                    "trace_id": None,
                    "error": None,
                    "retry_count": 0,
                    "max_retries": 3
                }
                
                # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
                config = {"configurable": {"thread_id": f"eval_{idx}"}}
                final_state = self.workflow.invoke(initial_state, config)
                
                # ê²°ê³¼ ì²˜ë¦¬
                if final_state.get("error"):
                    print(f"   âŒ ì˜¤ë¥˜: {final_state['error']}")
                    results.append({
                        "item_index": idx,
                        "error": final_state["error"],
                        "results": []
                    })
                else:
                    print(f"   âœ… ì™„ë£Œ: {len(final_state['evaluation_results'])}ê°œ í„´")
                    results.append({
                        "item_index": idx,
                        "error": None,
                        "results": final_state["evaluation_results"]
                    })
                    
            except Exception as e:
                print(f"   âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
                results.append({
                    "item_index": idx,
                    "error": str(e),
                    "results": []
                })
        
        return results
    
    def _run_parallel_evaluation(self, items: List, run_name: str, max_workers: int) -> List[Dict]:
        """ë³‘ë ¬ í‰ê°€ ì‹¤í–‰"""
        def evaluate_item(item_data):
            item, idx = item_data
            try:
                initial_state = {
                    "dataset_item": item,
                    "item_index": idx,
                    "run_name": run_name,
                    "messages": [],
                    "evaluation_results": [],
                    "current_turn": 1,
                    "max_turns": 0,
                    "trace_id": None,
                    "error": None,
                    "retry_count": 0,
                    "max_retries": 3
                }
                
                config = {"configurable": {"thread_id": f"eval_{idx}"}}
                final_state = self.workflow.invoke(initial_state, config)
                
                if final_state.get("error"):
                    return {
                        "item_index": idx,
                        "error": final_state["error"],
                        "results": []
                    }
                else:
                    return {
                        "item_index": idx,
                        "error": None,
                        "results": final_state["evaluation_results"]
                    }
                    
            except Exception as e:
                return {
                    "item_index": idx,
                    "error": str(e),
                    "results": []
                }
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            item_data = [(item, idx) for idx, item in enumerate(items, 1)]
            results = list(tqdm(
                executor.map(evaluate_item, item_data),
                total=len(items),
                desc="í‰ê°€ ì§„í–‰"
            ))
        
        return results
    
    def save_results_to_file(self, results: List[Dict], output_path: str):
        """í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            serializable_results = []
            for result_item in results:
                if result_item.get("error"):
                    serializable_results.append({
                        "item_index": result_item["item_index"],
                        "error": result_item["error"],
                        "results": []
                    })
                else:
                    item_dict = {
                        "item_index": result_item["item_index"],
                        "error": None,
                        "results": []
                    }
                    
                    for result in result_item["results"]:
                        result_dict = {
                            "item_id": result.item_id,
                            "key": result.key,
                            "turn_index": result.turn_index,
                            "input": result.input.prompt,
                            "output": result.output,
                            "trace_id": result.trace_id,
                            "error": result.error
                        }
                        
                        if result.score:
                            result_dict["score"] = asdict(result.score)
                        
                        if result.result_strict:
                            result_dict["result_strict"] = asdict(result.result_strict)
                        
                        if result.result_loose:
                            result_dict["result_loose"] = asdict(result.result_loose)
                        
                        item_dict["results"].append(result_dict)
                    
                    serializable_results.append(item_dict)
            
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
            
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Langfuseì™€ LangGraphë¥¼ í™œìš©í•œ ë©€í‹°í„´ Instruction Following í‰ê°€")
    parser.add_argument("--dataset", "-d", type=str, required=True, help="í‰ê°€í•  Langfuse ë°ì´í„°ì…‹ ì´ë¦„")
    parser.add_argument("--run-name", "-r", type=str, default=f"langgraph_ifeval_{int(time.time())}", help="í‰ê°€ ì‹¤í–‰ ì´ë¦„")
    parser.add_argument("--model", "-m", type=str, help="ì‚¬ìš©í•  LLM ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--temperature", "-t", type=float, default=0.6, help="ìƒì„± ì˜¨ë„")
    parser.add_argument("--limit", "-l", type=int, help="í‰ê°€í•  ìµœëŒ€ ì•„ì´í…œ ìˆ˜")
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
    parser.add_argument("--output", "-o", type=str, help="ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--parallel", "-p", action="store_true", help="ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©")
    parser.add_argument("--workers", "-w", type=int, default=4, help="ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—…ì ìˆ˜")
    
    args = parser.parse_args()
    
    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = LangGraphEvaluator(
        model_name=args.model,
        temperature=args.temperature,
        verbose=args.verbose
    )
    
    # í‰ê°€ ì‹¤í–‰
    results = evaluator.run_dataset_evaluation(
        dataset_name=args.dataset,
        run_name=args.run_name,
        limit=args.limit,
        parallel=args.parallel,
        max_workers=args.workers
    )
    
    # ê²°ê³¼ ì €ì¥
    if args.output:
        evaluator.save_results_to_file(results, args.output)
    
    print("âœ… LangGraph í‰ê°€ ì™„ë£Œ")

if __name__ == "__main__":
    main()